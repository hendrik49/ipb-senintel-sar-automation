{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growth stage classification by random forest, for a chosen date ('search date') over a specified area ('extent')\n",
    "\n",
    "1. Specify parameters , 'search date' and 'extent (i.e., polygon: lampung)\n",
    "\n",
    "    a. the usage of 'extent' is the same as that of vegetable classification (What is extent? Gjson?)\n",
    "\n",
    "    b. String of Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Search the corresponding S1A SAR data, and download and preprocess them.\n",
    "\n",
    "    This part is the same as that of vegetable classification. The only difference is that we only download and preprocess S1A SAR. Data that are close to the search date ('dateStr'), e.g., S1A data on'20190430' and S1A data on '20190418'\n",
    "\n",
    "   Q1: From Data in Vegetable classification, we take 2 sample? is it correct?\n",
    "   Q2: Need stack or not? base on searchdate?\n",
    "   Q3: search date is random or must have sequential patern\n",
    "   \n",
    "   Then, we obtain, 2 file with img extension:\n",
    "   \n",
    "   img_current, and \n",
    "   img_previous \n",
    "   \n",
    "   The code will resul variable img_current and img_previous, where each image has two channels, VH and VV, \n",
    "    \n",
    "    e.g.:\n",
    "    - img_current(:,:,1) refers to the VH channel, while img_current(:,:,2) refers to VV channel in current date\n",
    "    - img_previous(:,:,1) refers to the VH channel, while img_preious(:,:,2) refers to VV channel in previous date\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide TsList\n",
    "import os, datetime\n",
    "from datetime import timedelta, date\n",
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from spectral import *\n",
    "from __future__ import print_function\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "home = os.getcwd()\n",
    "folder= home+r'\\Validation_of_products_2018\\\\S1A_timeseries\\\\Lampung_S1A_timeseries_2018Anual_Medium.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built in Function\n",
    "\n",
    "def nearest_date(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "def get_list_date(files):\n",
    "    temp = files.split('.')[0].split('_')\n",
    "    return datetime.datetime.strptime(temp[len(temp)-1], '%d%b%Y')\n",
    "\n",
    "def find_files(list_date, files, ext, j, dateStr, daysInterval):\n",
    "    date_curr = nearest_date(list_date[ext[j]], datetime.datetime.strptime(str(dateStr), '%d%b%Y'))\n",
    "    date_prev = date_curr - datetime.timedelta(daysInterval)\n",
    "    idx_curr = list_date[ext[j]].index(date_curr)\n",
    "    idx_prev = list_date[ext[j]].index(date_prev)\n",
    "    return files[ext[j]][idx_curr], files[ext[j]][idx_prev]\n",
    "\n",
    "def get_img_file(img_file, hdr_file, val):\n",
    "    hdr_vh = folder +'/'+hdr_file[0][val]\n",
    "    img_vh = folder +'/'+img_file[0][val]\n",
    "    hdr_vv = folder +'/'+hdr_file[1][val]\n",
    "    img_vv = folder +'/'+img_file[1][val]\n",
    "    l = [[hdr_vh, img_vh], [hdr_vv, img_vv]]\n",
    "    if Path(hdr_vh).is_file() and Path(img_vh).is_file():\n",
    "        if Path(hdr_vv).is_file() and Path(img_vv).is_file():\n",
    "            get_info = envi.read_envi_header(hdr_vh)\n",
    "            d1 = int(get_info['lines'])\n",
    "            d2 = int(get_info['samples'])\n",
    "            dim = (d1,d2,2)\n",
    "            im_subset = np.zeros((dim))\n",
    "    for ix in tqdm(range(len(l))):\n",
    "        get_img = envi.open(l[ix][0])\n",
    "        img_open = get_img.open_memmap(writeable = True)\n",
    "        im = img_open[:d1,:d2,0]\n",
    "        im_subset[:,:,ix] = im\n",
    "    return im_subset\n",
    "\n",
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    function that normalizes each row of the matrix x to have unit length.\n",
    "\n",
    "    Args:\n",
    "     ``x``: A numpy matrix of shape (n, m)\n",
    "\n",
    "    Returns:\n",
    "     ``x``: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    return x/np.linalg.norm(x, ord=2, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "folder\n",
    "dateStr = '27Dec2018';\n",
    "ext1 = 'img'\n",
    "ext2 = 'hdr'\n",
    "splitStr       = '_'\n",
    "daysInterval   = 12\n",
    "time_formatStr = \"%d-%b-%Y\"\n",
    "bandName = ['Sigma0_VH','Sigma0_VV']\n",
    "folder= home+r'/Validation_of_products_2018/S1A_timeseries/Lampung_S1A_timeseries_2018Anual_Medium.data'\n",
    "train_path=home+r'/Validation_of_products_2018/train_mat.mat'\n",
    "ext = ['vh', 'vv']\n",
    "numfiles = len(filter(lambda x: \"Sigma0_\" in x, os.listdir(folder)))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dictionary:\n",
    "img = {\n",
    "    'vh': [f for f in os.listdir(folder) if f.endswith('.' + 'img') and \"_VH\" in f], \n",
    "    'vv': [f for f in os.listdir(folder) if f.endswith('.' + 'img') and \"_VV\" in f]}\n",
    "hdr = {\n",
    "    'vh': [f for f in os.listdir(folder) if f.endswith('.' + 'hdr') and \"_VH\" in f],\n",
    "    'vv': [f for f in os.listdir(folder) if f.endswith('.' + 'hdr') and \"_VV\" in f]}\n",
    "\n",
    "date_img = {\n",
    "    'vh': [get_list_date(i) for i in img[ext[0]]], \n",
    "    'vv': [get_list_date(i) for i in img[ext[1]]] }\n",
    "\n",
    "date_hdr = {\n",
    "    'vh': [get_list_date(i) for i in hdr[ext[0]]], \n",
    "    'vv': [get_list_date(i) for i in hdr[ext[1]]] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sigma0_VH_db_slv43_27Dec2018.img', 'Sigma0_VH_db_slv41_15Dec2018.img'),\n",
       " ('Sigma0_VV_db_slv44_27Dec2018.img', 'Sigma0_VV_db_slv42_15Dec2018.img')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file = [find_files(date_img, img, ext, j, \n",
    "                       dateStr, daysInterval) for j in range(len(date_img))]\n",
    "hdr_file = [find_files(date_hdr, hdr, ext, j, \n",
    "                       dateStr, daysInterval) for j in range(len(date_hdr))]\n",
    "img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sigma0_VH_db_slv43_27Dec2018.img', 'Sigma0_VH_db_slv41_15Dec2018.img'),\n",
       " ('Sigma0_VV_db_slv44_27Dec2018.img', 'Sigma0_VV_db_slv42_15Dec2018.img')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "img_current = get_img_file(img_file, hdr_file, 0) ## 0 for current, 1, previous\n",
    "img_previous = get_img_file(img_file, hdr_file, 1) ## 0 for current, 1, previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract features\n",
    "\n",
    "    Here, we extract, 6 features: \n",
    "    \n",
    "    - VH_img_current, \n",
    "    \n",
    "    - VV_img_current,\n",
    "    \n",
    "    - VH_img_current/VV_img_current, \n",
    "    \n",
    "    - VH_img_current-VH_img_previous,\n",
    "    \n",
    "    - VV_img_current-VV_img_previous, and\n",
    "    \n",
    "    - (VH_img_current/VV_img_current)-(VH_img_previous/VV_img_previous)\n",
    "   \n",
    "   then we will obtain : img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = np.zeros((img_current.shape[0], img_current.shape[1], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features[:,:,0] = img_current[:,:,0]\n",
    "img_features[:,:,1] = img_current[:,:,1]\n",
    "img_features[:,:,2] = img_current[:,:,0]/img_current[:,:,1]\n",
    "img_features[:,:,3] = img_current[:,:,0]-img_previous[:,:,0]\n",
    "img_features[:,:,4] = img_current[:,:,1]-img_previous[:,:,1]\n",
    "img_features[:,:,5] = img_current[:,:,0]/img_current[:,:,1]-\\\n",
    "                      img_previous[:,:,0]/img_previous[:,:,1]\n",
    "[d1,d2,d3] = img_features.shape\n",
    "# where d1, d2, d3 refer to the row, colum, and feature dimension, and d3=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4058, 7760, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.reshape(img_features,[d1*d2,d3]);\n",
    "test_features = normalize_rows(test_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.859008</td>\n",
       "      <td>-0.479902</td>\n",
       "      <td>0.098019</td>\n",
       "      <td>-0.124099</td>\n",
       "      <td>-0.082341</td>\n",
       "      <td>-0.003208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.864854</td>\n",
       "      <td>-0.478204</td>\n",
       "      <td>0.096384</td>\n",
       "      <td>-0.091513</td>\n",
       "      <td>-0.075168</td>\n",
       "      <td>-0.005875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.854256</td>\n",
       "      <td>-0.503531</td>\n",
       "      <td>0.091691</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>-0.084544</td>\n",
       "      <td>-0.014556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.865360</td>\n",
       "      <td>-0.491209</td>\n",
       "      <td>0.096381</td>\n",
       "      <td>-0.011458</td>\n",
       "      <td>-0.020876</td>\n",
       "      <td>-0.002945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.808251</td>\n",
       "      <td>-0.573741</td>\n",
       "      <td>0.072627</td>\n",
       "      <td>-0.027058</td>\n",
       "      <td>-0.106585</td>\n",
       "      <td>-0.013584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0 -0.859008 -0.479902  0.098019 -0.124099 -0.082341 -0.003208\n",
       "1 -0.864854 -0.478204  0.096384 -0.091513 -0.075168 -0.005875\n",
       "2 -0.854256 -0.503531  0.091691 -0.030586 -0.084544 -0.014556\n",
       "3 -0.865360 -0.491209  0.096381 -0.011458 -0.020876 -0.002945\n",
       "4 -0.808251 -0.573741  0.072627 -0.027058 -0.106585 -0.013584"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_features).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat(train_path)  # load mat-file\n",
    "mdata = mat['train_mat']  # variable in mat file\n",
    "mtype = mdata.dtype\n",
    "ndata = {n: mdata[n][0,0] for n in mtype.names}\n",
    "data_headline = ndata['feature_name']\n",
    "headline = data_headline[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = ndata['train_data']\n",
    "data_df = np.reshape(data_raw,[250,6]);\n",
    "data_df = normalize_rows(data_df);\n",
    "data_df = pd.DataFrame(data_raw)\n",
    "#data_df.columns=['Sigma0_VH','Sigma0_VV','VH_minus_VV_dB','diff_1','diff_2','diff_3']\n",
    "\n",
    "data_test = pd.DataFrame(ndata['train_label'])\n",
    "#data_test.columns=['class']\n",
    "\n",
    "train_features = data_df\n",
    "train_labels = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# Create random data train and labels 120x5, with n labels =3\n",
    "#X, y = make_blobs(n_samples=120, centers=3, n_features=6,random_state=1)\n",
    "#train_features = X\n",
    "#train_labels = y\n",
    "\n",
    "#normalize train data\n",
    "#train_features = normalize_rows(train_features);\n",
    "\n",
    "#pd.DataFrame(train_features).head()\n",
    "#train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8905615464285714"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels.values.ravel());\n",
    "rf.score(train_features, train_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  2.003\n",
       "1  2.003\n",
       "2  2.005\n",
       "3  2.005\n",
       "4  2.000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict result\n",
    "pd.DataFrame(predictions).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "#print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train random forest classifier, and do classification\n",
    "\n",
    "   You can use 'sklearn.ensemble.RandomForestClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. Training Data set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_cls = 4;\n",
    "num_obs_each_cls = 30;\n",
    "\n",
    "train_labels = np.zeros((4*30,1)); \n",
    "train_features = np.zeros((4*30,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import make_classification\n",
    "\n",
    "#X, y = make_classification(n_samples=120, n_features=6, n_informative=3, n_classes=3,\n",
    "#                           n_redundant=0, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_df\n",
    "y = data_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc =  RandomForestClassifier(n_estimators = 1000, random_state = 42, max_depth=15)\n",
    "rfc.fit(X_train, y_train.values.ravel());\n",
    "rfc.score(X_train, y_train.values.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0  3  0]\n",
      " [ 2  0  0  0]\n",
      " [ 2  0 10  0]\n",
      " [ 0  0  1  4]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.43      0.50      0.46         6\n",
      "          2       0.00      0.00      0.00         2\n",
      "          3       0.71      0.83      0.77        12\n",
      "          4       1.00      0.80      0.89         5\n",
      "\n",
      "avg / total       0.65      0.68      0.66        25\n",
      "\n",
      "0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions_clf = rfc.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame(predictions_clf)\n",
    "df_predict.columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3aaf935e6aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
